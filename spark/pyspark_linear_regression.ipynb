{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6a35eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import psutil # You might need to install this: pip install psutil\n",
    "\n",
    "# This SparkSession is already initialized when launching with pyspark command\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f549605-bf7a-4f08-87d7-30def2536333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0-preview2\n"
     ]
    }
   ],
   "source": [
    "# If you want to explicitly create it:\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClassificationExampleWithMetrics\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check if Spark is working\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780a1369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Dataset has 83459 rows and 20 columns\n",
      "Column names: ['vendor_id', 'pickup_time', 'dropoff_time', 'flag', 'rate_code', 'pu_location', 'do_location', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'trip_duration_minutes', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
      "Schema:\n",
      "root\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- pickup_time: timestamp (nullable = true)\n",
      " |-- dropoff_time: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- pu_location: integer (nullable = true)\n",
      " |-- do_location: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- trip_duration_minutes: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n",
      "+---------+-------------------+-------------------+----+---------+-----------+-----------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+---------------------+------------+------------+---------+--------------------+\n",
      "|vendor_id|        pickup_time|       dropoff_time|flag|rate_code|pu_location|do_location|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|trip_duration_minutes|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+---------+-------------------+-------------------+----+---------+-----------+-----------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+---------------------+------------+------------+---------+--------------------+\n",
      "|        1|2021-07-01 00:30:52|2021-07-01 00:35:36|   N|        1|         74|        168|              1|          1.2|        6.0|  0.5|    0.5|       0.0|         0.0|    4.733333333333333|                  0.3|         7.3|           2|        1|                 0.0|\n",
      "|        2|2021-07-01 00:25:36|2021-07-01 01:01:31|   N|        1|        116|        265|              2|        13.69|       42.0|  0.5|    0.5|       0.0|         0.0|   35.916666666666664|                  0.3|        43.3|           2|        1|                 0.0|\n",
      "|        2|2021-07-01 00:05:58|2021-07-01 00:12:00|   N|        1|         97|         33|              1|         0.95|        6.5|  0.5|    0.5|      2.34|         0.0|    6.033333333333333|                  0.3|       10.14|           1|        1|                 0.0|\n",
      "|        2|2021-07-01 00:41:40|2021-07-01 00:47:23|   N|        1|         74|         42|              1|         1.24|        6.5|  0.5|    0.5|       0.0|         0.0|    5.716666666666667|                  0.3|         7.8|           2|        1|                 0.0|\n",
      "|        2|2021-07-01 00:51:32|2021-07-01 00:58:46|   N|        1|         42|        244|              1|          1.1|        7.0|  0.5|    0.5|       0.0|         0.0|    7.233333333333333|                  0.3|         8.3|           2|        1|                 0.0|\n",
      "+---------+-------------------+-------------------+----+---------+-----------+-----------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Selected features for model: ['rate_code', 'pu_location', 'do_location', 'passenger_count', 'trip_distance', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'trip_duration_minutes', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
      "Final features: ['rate_code', 'pu_location', 'do_location', 'passenger_count', 'trip_distance', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'trip_duration_minutes', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
      "Training set: 40822 rows\n",
      "Test set: 10137 rows\n",
      "Training Time: 4.8553 seconds\n",
      "Prediction Time: 0.1263 seconds\n",
      "Sample Predictions:\n",
      "+-----------+------------------+\n",
      "|fare_amount|        prediction|\n",
      "+-----------+------------------+\n",
      "|       10.2| 9.891702056820044|\n",
      "|        7.5|7.7620314372397505|\n",
      "|        4.5| 4.463808728798631|\n",
      "|        7.0| 6.902585094199343|\n",
      "|       13.2|12.782084146293826|\n",
      "+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.3760\n",
      "R² on test data = 0.9993\n",
      "\n",
      "Linear Regression Model:\n",
      "Intercept: -0.1429\n",
      "\n",
      "Feature Coefficients (absolute value sorted):\n",
      "improvement_surcharge: 3.5411\n",
      "mta_tax: -2.5399\n",
      "tip_amount: -0.9604\n",
      "total_amount: 0.9591\n",
      "tolls_amount: -0.9269\n",
      "congestion_surcharge: -0.9020\n",
      "extra: -0.4300\n",
      "trip_type: -0.2198\n",
      "trip_distance: 0.1042\n",
      "payment_type: -0.0639\n",
      "Driver Memory Usage: 120.80 MB\n"
     ]
    }
   ],
   "source": [
    "# Load data.csv and create a regression model to predict total_amount\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression  # Change import to LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "data_path = \"data.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True, nullValue=\"\\\\N\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset has {df.count()} rows and {len(df.columns)} columns\")\n",
    "print(\"Column names:\", df.columns)\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# Drop any rows with null values\n",
    "df = df.na.drop()\n",
    "\n",
    "# Explicitly cast the target column to DoubleType for regression\n",
    "df = df.withColumn(\"fare_amount\", df[\"fare_amount\"].cast(DoubleType()))\n",
    "\n",
    "# Exclude non-relevant columns\n",
    "exclude_columns = [\"vendor_id\", \"pickup_time\", \"dropoff_time\", \"flag\", \"fare_amount\"]  # Target and columns to exclude\n",
    "feature_cols = [col for col in df.columns if col not in exclude_columns]\n",
    "\n",
    "print(\"Selected features for model:\", feature_cols)\n",
    "\n",
    "# Cast all numeric features to double to ensure compatibility\n",
    "for col in feature_cols:\n",
    "    if col in df.columns:  # Make sure the column exists\n",
    "        df = df.withColumn(col, df[col].cast(DoubleType()))\n",
    "\n",
    "# Create pipeline stages for preprocessing\n",
    "stages = []\n",
    "\n",
    "# Convert any remaining string columns to numeric\n",
    "string_columns = [field.name for field in df.schema.fields \n",
    "                  if field.dataType.simpleString().startswith(\"string\") \n",
    "                  and field.name in feature_cols]\n",
    "\n",
    "# Check if there are any string columns that need processing\n",
    "if string_columns:\n",
    "    for col in string_columns:\n",
    "        # First convert strings to indices\n",
    "        indexer = StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\")\n",
    "        # Then convert indices to one-hot encoding\n",
    "        encoder = OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_encoded\")\n",
    "        stages.extend([indexer, encoder])\n",
    "        # Update feature columns list, replacing original string column with encoded version\n",
    "        feature_cols.remove(col)\n",
    "        feature_cols.append(f\"{col}_encoded\")\n",
    "\n",
    "# Prepare numeric features \n",
    "numeric_cols = [field.name for field in df.schema.fields \n",
    "               if not field.dataType.simpleString().startswith(\"string\")\n",
    "               and field.name in feature_cols]\n",
    "\n",
    "# Make sure we have features to work with\n",
    "final_features = numeric_cols + [f\"{col}_encoded\" for col in string_columns]\n",
    "if not final_features:\n",
    "    raise ValueError(\"No features available for training after preprocessing\")\n",
    "    \n",
    "print(\"Final features:\", final_features)\n",
    "\n",
    "# Add vector assembler to pipeline stages\n",
    "assembler = VectorAssembler(inputCols=final_features, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "stages.append(assembler)\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set: {train_data.count()} rows\")\n",
    "print(f\"Test set: {test_data.count()} rows\")\n",
    "\n",
    "# Replace the Random Forest regressor with Linear Regression\n",
    "# Create Linear Regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"fare_amount\",\n",
    "    maxIter=10,       # Maximum iterations\n",
    "    regParam=0.1,     # Regularization parameter\n",
    "    elasticNetParam=0.0  # ElasticNet mixing parameter (0 = Ridge, 1 = Lasso)\n",
    ")\n",
    "stages.append(lr)\n",
    "\n",
    "# Create the pipeline with all stages\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Try to fit the model with error handling\n",
    "try:\n",
    "    # --- Measure Training Time ---\n",
    "    start_train_time = time.time()\n",
    "    model = pipeline.fit(train_data)\n",
    "    end_train_time = time.time()\n",
    "    training_time = end_train_time - start_train_time\n",
    "    print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "    \n",
    "    # Continue with the rest of your code...\n",
    "    # --- Measure Prediction Time ---\n",
    "    start_pred_time = time.time()\n",
    "    predictions = model.transform(test_data)\n",
    "    end_pred_time = time.time()\n",
    "    prediction_time = end_pred_time - start_pred_time\n",
    "    print(f\"Prediction Time: {prediction_time:.4f} seconds\")\n",
    "    # --- End Prediction Time Measurement ---\n",
    "\n",
    "    # Select example rows to display\n",
    "    print(\"Sample Predictions:\")\n",
    "    predictions.select(\"fare_amount\", \"prediction\").show(5)\n",
    "\n",
    "    # --- Evaluate Model ---\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"fare_amount\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"Root Mean Squared Error (RMSE) on test data = {rmse:.4f}\")\n",
    "\n",
    "    # Also calculate R2 (coefficient of determination)\n",
    "    evaluator.setMetricName(\"r2\")\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "    print(f\"R² on test data = {r2:.4f}\")\n",
    "    # --- End Evaluation ---\n",
    "\n",
    "    # --- Feature Importance ---\n",
    "    # Extract coefficients from the linear model\n",
    "    coefficients = model.stages[-1].coefficients\n",
    "    intercept = model.stages[-1].intercept\n",
    "    print(f\"\\nLinear Regression Model:\")\n",
    "    print(f\"Intercept: {intercept:.4f}\")\n",
    "\n",
    "    # Display coefficients as feature importance\n",
    "    features_with_importance = list(zip(final_features, coefficients))\n",
    "    sorted_features = sorted(features_with_importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    print(\"\\nFeature Coefficients (absolute value sorted):\")\n",
    "    for feature, coef in sorted_features[:10]:  # Top 10 features\n",
    "        print(f\"{feature}: {coef:.4f}\")\n",
    "    # --- End Feature Importance ---\n",
    "\n",
    "    # --- Measure Memory Usage (Driver Process) ---\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    memory_usage_mb = memory_info.rss / (1024 * 1024)  # Resident Set Size in MB\n",
    "    print(f\"Driver Memory Usage: {memory_usage_mb:.2f} MB\")\n",
    "    # --- End Memory Usage Measurement ---\n",
    "except Exception as e:\n",
    "    print(f\"Error during model training: {str(e)}\")\n",
    "    \n",
    "    # Try to provide more diagnostic information\n",
    "    print(\"\\nDiagnostic information:\")\n",
    "    print(f\"Number of features: {len(final_features)}\")\n",
    "    \n",
    "    # Check for nulls in features\n",
    "    for col in final_features:\n",
    "        null_count = df.filter(df[col].isNull()).count()\n",
    "        if null_count > 0:\n",
    "            print(f\"Column '{col}' has {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e686e7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SPARK APPLICATION INFORMATION =====\n",
      "Application Name: ClassificationExampleWithMetrics\n",
      "Spark Version: 4.0.0-preview2\n",
      "Master: local[*]\n",
      "\n",
      "===== CLUSTER RESOURCES =====\n",
      "Number of executors: 1\n",
      "Default Parallelism: 8\n",
      "Available Cores: 8\n",
      "\n",
      "===== SPARK CONFIGURATION =====\n",
      "spark.app.id: local-1746844806252\n",
      "spark.app.name: ClassificationExampleWithMetrics\n",
      "spark.app.startTime: 1746844804731\n",
      "spark.app.submitTime: 1746844802665\n",
      "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "spark.driver.host: e9ae393c1ce6\n",
      "spark.driver.port: 45729\n",
      "spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "spark.executor.id: driver\n",
      "spark.hadoop.fs.s3a.connection.establish.timeout: 30000\n",
      "spark.master: local[*]\n",
      "spark.rdd.compress: True\n",
      "spark.serializer.objectStreamReset: 100\n",
      "spark.sql.warehouse.dir: file:/home/jovyan/work/spark-warehouse\n",
      "spark.submit.deployMode: client\n",
      "spark.submit.pyFiles: \n",
      "spark.ui.showConsoleProgress: true\n",
      "\n",
      "===== MEMORY INFORMATION =====\n",
      "Driver Memory: Not set\n",
      "Executor Memory: Not set\n",
      "\n",
      "===== ML PIPELINE INFORMATION =====\n",
      "Pipeline has 2 stages:\n",
      "  Stage 0: VectorAssembler\n",
      "  Stage 1: LinearRegressionModel\n",
      "    Coefficients shape: 15\n",
      "    Intercept: -0.1429\n",
      "    Num iterations: 0\n",
      "    Training RMSE: 0.4073\n",
      "    Training R²: 0.9992\n",
      "\n",
      "===== EXECUTION METRICS =====\n",
      "Note: For more detailed metrics, view the Spark UI at http://localhost:4040\n",
      "\n",
      "===== PROCESS METRICS =====\n",
      "Process ID: 7333\n",
      "CPU Usage: 1.0%\n",
      "Memory Usage (RSS): 120.80 MB\n",
      "Memory Usage (VMS): 1885.61 MB\n",
      "\n",
      "===== PERFORMANCE SUMMARY =====\n",
      "Model Training Time: 4.8553 seconds\n",
      "Prediction Time: 0.1263 seconds\n",
      "RMSE: 0.3760\n",
      "R²: 0.9993\n"
     ]
    }
   ],
   "source": [
    "# Get details about ML tasks and cluster resources\n",
    "\n",
    "# 1. Spark application information\n",
    "print(\"===== SPARK APPLICATION INFORMATION =====\")\n",
    "print(f\"Application Name: {spark.conf.get('spark.app.name')}\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.conf.get('spark.master')}\")\n",
    "\n",
    "# 2. Cluster resources\n",
    "print(\"\\n===== CLUSTER RESOURCES =====\")\n",
    "print(f\"Number of executors: {spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size()}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Available Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "# 3. Spark configuration\n",
    "print(\"\\n===== SPARK CONFIGURATION =====\")\n",
    "spark_config = spark.sparkContext.getConf().getAll()\n",
    "for item in sorted(spark_config, key=lambda x: x[0]):\n",
    "    print(f\"{item[0]}: {item[1]}\")\n",
    "\n",
    "# 4. Memory information\n",
    "print(\"\\n===== MEMORY INFORMATION =====\")\n",
    "driver_memory = spark.sparkContext._jsc.sc().getConf().get(\"spark.driver.memory\", \"Not set\")\n",
    "executor_memory = spark.sparkContext._jsc.sc().getConf().get(\"spark.executor.memory\", \"Not set\")\n",
    "print(f\"Driver Memory: {driver_memory}\")\n",
    "print(f\"Executor Memory: {executor_memory}\")\n",
    "\n",
    "# 5. ML pipeline information (if model exists in this scope)\n",
    "if 'model' in locals():\n",
    "    print(\"\\n===== ML PIPELINE INFORMATION =====\")\n",
    "    stages = model.stages\n",
    "    print(f\"Pipeline has {len(stages)} stages:\")\n",
    "    for i, stage in enumerate(stages):\n",
    "        print(f\"  Stage {i}: {type(stage).__name__}\")\n",
    "        # For LinearRegressionModel, print more details\n",
    "        if hasattr(stage, 'coefficients'):\n",
    "            print(f\"    Coefficients shape: {stage.coefficients.size}\")\n",
    "            print(f\"    Intercept: {stage.intercept:.4f}\")\n",
    "            print(f\"    Num iterations: {stage.summary.totalIterations}\")\n",
    "            print(f\"    Training RMSE: {stage.summary.rootMeanSquaredError:.4f}\")\n",
    "            print(f\"    Training R²: {stage.summary.r2:.4f}\")\n",
    "\n",
    "# 6. Detailed execution metrics (UI information)\n",
    "print(\"\\n===== EXECUTION METRICS =====\")\n",
    "print(\"Note: For more detailed metrics, view the Spark UI at http://localhost:4040\")\n",
    "\n",
    "# 7. Process local metrics\n",
    "print(\"\\n===== PROCESS METRICS =====\")\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_info = process.memory_info()\n",
    "cpu_percent = process.cpu_percent(interval=1.0)\n",
    "print(f\"Process ID: {os.getpid()}\")\n",
    "print(f\"CPU Usage: {cpu_percent}%\")\n",
    "print(f\"Memory Usage (RSS): {memory_info.rss / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Memory Usage (VMS): {memory_info.vms / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# 8. Performance summary if available\n",
    "print(\"\\n===== PERFORMANCE SUMMARY =====\")\n",
    "if 'training_time' in locals():\n",
    "    print(f\"Model Training Time: {training_time:.4f} seconds\")\n",
    "if 'prediction_time' in locals():\n",
    "    print(f\"Prediction Time: {prediction_time:.4f} seconds\")\n",
    "if 'rmse' in locals() and 'r2' in locals():\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0082cf8-f747-4ab2-a574-a302f938bf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
