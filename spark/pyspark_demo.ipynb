{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b662559-acdc-45c2-b8c5-5448284f833c",
   "metadata": {},
   "source": [
    "# Pyspark Machine Learning\n",
    "\n",
    "Using `pyspark` on Docker exposed `localhost` port to run machine learning jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b4e00-4137-4e07-a0a6-a90815daeaca",
   "metadata": {},
   "source": [
    "## General Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362ca2c2-b08f-4d3a-b384-f4112f2f83ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0-preview2\n",
      "+---+-----+----------+\n",
      "| id|label|prediction|\n",
      "+---+-----+----------+\n",
      "|  0|  cat|       0.0|\n",
      "|  1|  dog|       1.0|\n",
      "|  2|  dog|       1.0|\n",
      "|  3|  cat|       0.0|\n",
      "+---+-----+----------+\n",
      "\n",
      "Test Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "# This SparkSession is already initialized when launching with pyspark command\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# If you want to explicitly create it:\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClassificationExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check if Spark is working\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Simple example with classification\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create sample data\n",
    "data = [(0, 1.0, 0.0, \"cat\"),\n",
    "        (1, 0.0, 1.0, \"dog\"),\n",
    "        (2, 0.0, 1.0, \"dog\"),\n",
    "        (3, 1.0, 0.0, \"cat\")]\n",
    "columns = [\"id\", \"feature1\", \"feature2\", \"label\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = [\"feature1\", \"feature2\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Convert string labels to indices\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "\n",
    "# Create and train model\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, label_indexer, rf])\n",
    "\n",
    "# Train model\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Select example rows to display\n",
    "predictions.select(\"id\", \"label\", \"prediction\").show()\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840992a1-b1c8-45e4-896a-a9e7ad1f1907",
   "metadata": {},
   "source": [
    "Note that after the cell is executed need to wait a while to get the result to be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7d8bf-5a36-415d-92e7-cb80bdc6afa1",
   "metadata": {},
   "source": [
    "## Include Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c08db5d-e302-498f-8907-1368ac1876e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0-preview2\n",
      "Training Time: 4.1157 seconds\n",
      "Prediction Time: 0.1216 seconds\n",
      "Sample Predictions:\n",
      "+---+-----+----------+------------+\n",
      "| id|label|prediction|indexedLabel|\n",
      "+---+-----+----------+------------+\n",
      "|  0|  cat|       0.0|         0.0|\n",
      "|  1|  dog|       1.0|         1.0|\n",
      "|  2|  dog|       1.0|         1.0|\n",
      "|  3|  cat|       0.0|         0.0|\n",
      "+---+-----+----------+------------+\n",
      "\n",
      "Test Accuracy = 1.0000\n",
      "Driver Memory Usage: 195.71 MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import psutil # You might need to install this: pip install psutil\n",
    "\n",
    "# This SparkSession is already initialized when launching with pyspark command\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# If you want to explicitly create it:\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClassificationExampleWithMetrics\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check if Spark is working\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Simple example with classification\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create sample data\n",
    "data = [(0, 1.0, 0.0, \"cat\"),\n",
    "        (1, 0.0, 1.0, \"dog\"),\n",
    "        (2, 0.0, 1.0, \"dog\"),\n",
    "        (3, 1.0, 0.0, \"cat\")]\n",
    "columns = [\"id\", \"feature1\", \"feature2\", \"label\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = [\"feature1\", \"feature2\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Convert string labels to indices\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "\n",
    "# Create model\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, label_indexer, rf])\n",
    "\n",
    "# --- Measure Training Time ---\n",
    "start_train_time = time.time()\n",
    "model = pipeline.fit(df)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "# --- End Training Time Measurement ---\n",
    "\n",
    "# --- Measure Prediction Time ---\n",
    "start_pred_time = time.time()\n",
    "predictions = model.transform(df)\n",
    "end_pred_time = time.time()\n",
    "prediction_time = end_pred_time - start_pred_time\n",
    "print(f\"Prediction Time: {prediction_time:.4f} seconds\")\n",
    "# --- End Prediction Time Measurement ---\n",
    "\n",
    "# Select example rows to display\n",
    "print(\"Sample Predictions:\")\n",
    "predictions.select(\"id\", \"label\", \"prediction\", \"indexedLabel\").show() # Added indexedLabel for clarity\n",
    "\n",
    "# --- Evaluate Model (Accuracy) ---\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
    "# --- End Evaluation ---\n",
    "\n",
    "# --- Measure Memory Usage (Driver Process) ---\n",
    "# Note: This measures the memory usage of the Python driver process,\n",
    "# not the total cluster memory used by Spark executors.\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_info = process.memory_info()\n",
    "memory_usage_mb = memory_info.rss / (1024 * 1024) # Resident Set Size in MB\n",
    "print(f\"Driver Memory Usage: {memory_usage_mb:.2f} MB\")\n",
    "# --- End Memory Usage Measurement ---\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd94060-971b-4a06-bf7f-952e570ec531",
   "metadata": {},
   "source": [
    "We can see that the training time, prediction time, and drive memory usage are being reported. We can use it to compare against the `scikit-learn` approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
